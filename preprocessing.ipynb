{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window as W\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField,ArrayType,LongType,StringType,DoubleType,IntegerType\n",
    "from pyspark.sql.functions import split, explode,col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('earthquake_api').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = spark.read\\\n",
    "    .format('json')\\\n",
    "    .option('multiLine', 'true')\\\n",
    "    .load('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- alert: string (nullable = true)\n",
      " |    |    |-- cdi: string (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- continent: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- depth: string (nullable = true)\n",
      " |    |    |-- detailUrl: string (nullable = true)\n",
      " |    |    |-- dmin: string (nullable = true)\n",
      " |    |    |-- felt: string (nullable = true)\n",
      " |    |    |-- gap: string (nullable = true)\n",
      " |    |    |-- geometryType: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- ids: string (nullable = true)\n",
      " |    |    |-- latitude: string (nullable = true)\n",
      " |    |    |-- locality: string (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- locationDetails: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- adminLevel: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- geonameId: string (nullable = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- wikidataId: string (nullable = true)\n",
      " |    |    |-- longitude: string (nullable = true)\n",
      " |    |    |-- magType: string (nullable = true)\n",
      " |    |    |-- magnitude: string (nullable = true)\n",
      " |    |    |-- mmi: string (nullable = true)\n",
      " |    |    |-- net: string (nullable = true)\n",
      " |    |    |-- nst: string (nullable = true)\n",
      " |    |    |-- place: string (nullable = true)\n",
      " |    |    |-- postcode: string (nullable = true)\n",
      " |    |    |-- rms: string (nullable = true)\n",
      " |    |    |-- sig: string (nullable = true)\n",
      " |    |    |-- sources: string (nullable = true)\n",
      " |    |    |-- status: string (nullable = true)\n",
      " |    |    |-- subnational: string (nullable = true)\n",
      " |    |    |-- time: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- tsunami: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- types: string (nullable = true)\n",
      " |    |    |-- updated: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- what3words: string (nullable = true)\n",
      " |-- errorCode: string (nullable = true)\n",
      " |-- errors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- friendlyError: string (nullable = true)\n",
      " |-- httpStatus: long (nullable = true)\n",
      " |-- noun: string (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      " |-- verb: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "earthquake_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+------+-------------+----------+-----------+-------+----+\n",
      "|count|                data|errorCode|errors|friendlyError|httpStatus|       noun| result|verb|\n",
      "+-----+--------------------+---------+------+-------------+----------+-----------+-------+----+\n",
      "|  100|[{, 3, , 7000iqb8...|     none|    []|             |       200|earthquakes|success| GET|\n",
      "+-----+--------------------+---------+------+-------------+----------+-----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = earthquake_df.drop(\"count\").drop(\"errorCode\").drop(\"errors\").drop(\"friendlyError\").drop(\"httpStatus\").drop(\"noun\").drop( \"result\").drop(\"verb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- alert: string (nullable = true)\n",
      " |    |    |-- cdi: string (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- continent: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- depth: string (nullable = true)\n",
      " |    |    |-- detailUrl: string (nullable = true)\n",
      " |    |    |-- dmin: string (nullable = true)\n",
      " |    |    |-- felt: string (nullable = true)\n",
      " |    |    |-- gap: string (nullable = true)\n",
      " |    |    |-- geometryType: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- ids: string (nullable = true)\n",
      " |    |    |-- latitude: string (nullable = true)\n",
      " |    |    |-- locality: string (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- locationDetails: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- adminLevel: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- geonameId: string (nullable = true)\n",
      " |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- wikidataId: string (nullable = true)\n",
      " |    |    |-- longitude: string (nullable = true)\n",
      " |    |    |-- magType: string (nullable = true)\n",
      " |    |    |-- magnitude: string (nullable = true)\n",
      " |    |    |-- mmi: string (nullable = true)\n",
      " |    |    |-- net: string (nullable = true)\n",
      " |    |    |-- nst: string (nullable = true)\n",
      " |    |    |-- place: string (nullable = true)\n",
      " |    |    |-- postcode: string (nullable = true)\n",
      " |    |    |-- rms: string (nullable = true)\n",
      " |    |    |-- sig: string (nullable = true)\n",
      " |    |    |-- sources: string (nullable = true)\n",
      " |    |    |-- status: string (nullable = true)\n",
      " |    |    |-- subnational: string (nullable = true)\n",
      " |    |    |-- time: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- tsunami: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- types: string (nullable = true)\n",
      " |    |    |-- updated: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- what3words: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = earthquake_df.withColumn('data', F.explode('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- alert: string (nullable = true)\n",
      " |    |-- cdi: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- code: string (nullable = true)\n",
      " |    |-- continent: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- depth: string (nullable = true)\n",
      " |    |-- detailUrl: string (nullable = true)\n",
      " |    |-- dmin: string (nullable = true)\n",
      " |    |-- felt: string (nullable = true)\n",
      " |    |-- gap: string (nullable = true)\n",
      " |    |-- geometryType: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- ids: string (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- locality: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- locationDetails: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- adminLevel: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- geonameId: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- wikidataId: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |    |-- magType: string (nullable = true)\n",
      " |    |-- magnitude: string (nullable = true)\n",
      " |    |-- mmi: string (nullable = true)\n",
      " |    |-- net: string (nullable = true)\n",
      " |    |-- nst: string (nullable = true)\n",
      " |    |-- place: string (nullable = true)\n",
      " |    |-- postcode: string (nullable = true)\n",
      " |    |-- rms: string (nullable = true)\n",
      " |    |-- sig: string (nullable = true)\n",
      " |    |-- sources: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- subnational: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |    |-- timezone: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- tsunami: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- types: string (nullable = true)\n",
      " |    |-- updated: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- what3words: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'data' does not exist. Did you mean one of the following? [date, id, place, status, time, title, type, updated, country, latitude, location, magnitude, continent, longitude];\n'Project ['data[id] AS id#572, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438]\n+- Project [id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438]\n   +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438, 100 AS counts#479]\n      +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, data#56.country AS country#438, counts#200]\n         +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, data#56.continent AS continent#421, country#175, counts#200]\n            +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, data#56.location AS location#404, continent#160, country#175, counts#200]\n               +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, data#56.place AS place#387, location#146, continent#160, country#175, counts#200]\n                  +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, cast(data#56.longitude as int) AS longitude#370, place#133, location#146, continent#160, country#175, counts#200]\n                     +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, cast(data#56.latitude as int) AS latitude#353, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                        +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, data#56.status AS status#336, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                           +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, cast(data#56.updated as int) AS updated#319, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                              +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, data#56.time AS time#302, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                 +- Project [data#56, id#217, magnitude#234, type#251, title#268, data#56.date AS date#285, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                    +- Project [data#56, id#217, magnitude#234, type#251, data#56.title AS title#268, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                       +- Project [data#56, id#217, magnitude#234, data#56.type AS type#251, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                          +- Project [data#56, id#217, cast(data#56.magnitude as int) AS magnitude#234, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                             +- Project [data#56, data#56.id AS id#217, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                                +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, 100 AS counts#200]\n                                                   +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, data#56.country AS country#175]\n                                                      +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, data#56.continent AS continent#160]\n                                                         +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, data#56.location AS location#146]\n                                                            +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, data#56.place AS place#133]\n                                                               +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, cast(data#56.longitude as int) AS longitude#121]\n                                                                  +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, cast(data#56.latitude as int) AS latitude#110]\n                                                                     +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, data#56.status AS status#100]\n                                                                        +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, cast(data#56.updated as int) AS updated#91]\n                                                                           +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, data#56.time AS time#83]\n                                                                              +- Project [data#56, id#58, magnitude#61, type#65, title#70, data#56.date AS date#76]\n                                                                                 +- Project [data#56, id#58, magnitude#61, type#65, data#56.title AS title#70]\n                                                                                    +- Project [data#56, id#58, magnitude#61, data#56.type AS type#65]\n                                                                                       +- Project [data#56, id#58, cast(data#56.magnitude as int) AS magnitude#61]\n                                                                                          +- Project [data#56, data#56.id AS id#58]\n                                                                                             +- Project [data#56]\n                                                                                                +- Generate explode(data#1), false, [data#56]\n                                                                                                   +- Project [data#1]\n                                                                                                      +- Project [data#1, verb#8]\n                                                                                                         +- Project [data#1, result#7, verb#8]\n                                                                                                            +- Project [data#1, noun#6, result#7, verb#8]\n                                                                                                               +- Project [data#1, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                  +- Project [data#1, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                     +- Project [data#1, errors#3, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                        +- Project [data#1, errorCode#2, errors#3, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                           +- Relation [count#0L,data#1,errorCode#2,errors#3,friendlyError#4,httpStatus#5L,noun#6,result#7,verb#8] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-36e29b07cffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mearthquake_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearthquake_df\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'magnitude'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'magnitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'data' does not exist. Did you mean one of the following? [date, id, place, status, time, title, type, updated, country, latitude, location, magnitude, continent, longitude];\n'Project ['data[id] AS id#572, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438]\n+- Project [id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438]\n   +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, country#438, 100 AS counts#479]\n      +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, continent#421, data#56.country AS country#438, counts#200]\n         +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, location#404, data#56.continent AS continent#421, country#175, counts#200]\n            +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, place#387, data#56.location AS location#404, continent#160, country#175, counts#200]\n               +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, longitude#370, data#56.place AS place#387, location#146, continent#160, country#175, counts#200]\n                  +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, latitude#353, cast(data#56.longitude as int) AS longitude#370, place#133, location#146, continent#160, country#175, counts#200]\n                     +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, status#336, cast(data#56.latitude as int) AS latitude#353, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                        +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, updated#319, data#56.status AS status#336, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                           +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, time#302, cast(data#56.updated as int) AS updated#319, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                              +- Project [data#56, id#217, magnitude#234, type#251, title#268, date#285, data#56.time AS time#302, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                 +- Project [data#56, id#217, magnitude#234, type#251, title#268, data#56.date AS date#285, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                    +- Project [data#56, id#217, magnitude#234, type#251, data#56.title AS title#268, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                       +- Project [data#56, id#217, magnitude#234, data#56.type AS type#251, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                          +- Project [data#56, id#217, cast(data#56.magnitude as int) AS magnitude#234, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                             +- Project [data#56, data#56.id AS id#217, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, counts#200]\n                                                +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, country#175, 100 AS counts#200]\n                                                   +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, continent#160, data#56.country AS country#175]\n                                                      +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, location#146, data#56.continent AS continent#160]\n                                                         +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, place#133, data#56.location AS location#146]\n                                                            +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, longitude#121, data#56.place AS place#133]\n                                                               +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, latitude#110, cast(data#56.longitude as int) AS longitude#121]\n                                                                  +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, status#100, cast(data#56.latitude as int) AS latitude#110]\n                                                                     +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, updated#91, data#56.status AS status#100]\n                                                                        +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, time#83, cast(data#56.updated as int) AS updated#91]\n                                                                           +- Project [data#56, id#58, magnitude#61, type#65, title#70, date#76, data#56.time AS time#83]\n                                                                              +- Project [data#56, id#58, magnitude#61, type#65, title#70, data#56.date AS date#76]\n                                                                                 +- Project [data#56, id#58, magnitude#61, type#65, data#56.title AS title#70]\n                                                                                    +- Project [data#56, id#58, magnitude#61, data#56.type AS type#65]\n                                                                                       +- Project [data#56, id#58, cast(data#56.magnitude as int) AS magnitude#61]\n                                                                                          +- Project [data#56, data#56.id AS id#58]\n                                                                                             +- Project [data#56]\n                                                                                                +- Generate explode(data#1), false, [data#56]\n                                                                                                   +- Project [data#1]\n                                                                                                      +- Project [data#1, verb#8]\n                                                                                                         +- Project [data#1, result#7, verb#8]\n                                                                                                            +- Project [data#1, noun#6, result#7, verb#8]\n                                                                                                               +- Project [data#1, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                  +- Project [data#1, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                     +- Project [data#1, errors#3, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                        +- Project [data#1, errorCode#2, errors#3, friendlyError#4, httpStatus#5L, noun#6, result#7, verb#8]\n                                                                                                                           +- Relation [count#0L,data#1,errorCode#2,errors#3,friendlyError#4,httpStatus#5L,noun#6,result#7,verb#8] json\n"
     ]
    }
   ],
   "source": [
    "earthquake_df = earthquake_df\\\n",
    "            .withColumn('id', F.col('data').getItem('id'))\\\n",
    "            .withColumn('magnitude', F.col('data').getItem('magnitude').cast('int'))\\\n",
    "            .withColumn('type', F.col('data').getItem('type'))\\\n",
    "            .withColumn('title', F.col('data').getItem('title'))\\\n",
    "    \t    .withColumn('date', F.col('data').getItem('date'))\\\n",
    "    \t    .withColumn('time', F.col('data').getItem('time'))\\\n",
    "    \t    .withColumn('updated', F.col('data').getItem('updated').cast('int'))\\\n",
    "    \t    .withColumn('status', F.col('data').getItem('status'))\\\n",
    "    \t    .withColumn('latitude', F.col('data').getItem('latitude').cast('int'))\\\n",
    "    \t    .withColumn('longitude', F.col('data').getItem('longitude').cast('int'))\\\n",
    "    \t    .withColumn('place', F.col('data').getItem('place'))\\\n",
    "    \t    .withColumn('location', F.col('data').getItem('location'))\\\n",
    "    \t    .withColumn('continent', F.col('data').getItem('continent'))\\\n",
    "    \t    .withColumn('country', F.col('data').getItem('country'))\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = earthquake_df\\\n",
    "                .select('id','magnitude', 'type', 'title', 'date', 'time', 'updated', 'status', 'latitude','longitude', 'place','location','continent','country')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"counts\" among (id, magnitude, type, title, date, time, updated, status, latitude, longitude, place, location, continent, country)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-90ae42df179d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mearthquake_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearthquake_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \"\"\"\n\u001b[1;32m   1964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"counts\" among (id, magnitude, type, title, date, time, updated, status, latitude, longitude, place, location, continent, country)"
     ]
    }
   ],
   "source": [
    "earthquake_df.select(earthquake_df['counts']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- alert: string (nullable = true)\n",
      " |    |-- cdi: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- code: string (nullable = true)\n",
      " |    |-- continent: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- depth: string (nullable = true)\n",
      " |    |-- detailUrl: string (nullable = true)\n",
      " |    |-- dmin: string (nullable = true)\n",
      " |    |-- felt: string (nullable = true)\n",
      " |    |-- gap: string (nullable = true)\n",
      " |    |-- geometryType: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- ids: string (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- locality: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- locationDetails: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- adminLevel: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- geonameId: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- wikidataId: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |    |-- magType: string (nullable = true)\n",
      " |    |-- magnitude: string (nullable = true)\n",
      " |    |-- mmi: string (nullable = true)\n",
      " |    |-- net: string (nullable = true)\n",
      " |    |-- nst: string (nullable = true)\n",
      " |    |-- place: string (nullable = true)\n",
      " |    |-- postcode: string (nullable = true)\n",
      " |    |-- rms: string (nullable = true)\n",
      " |    |-- sig: string (nullable = true)\n",
      " |    |-- sources: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- subnational: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |    |-- timezone: string (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- tsunami: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- types: string (nullable = true)\n",
      " |    |-- updated: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- what3words: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- magnitude: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- updated: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- latitude: integer (nullable = true)\n",
      " |-- longitude: integer (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- counts: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "earthquake_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation \n",
    "# 1. Find the average of magnitude of earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|avg(magnitude)|\n",
      "+--------------+\n",
      "|          3.36|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_magnitude_df = earthquake_df.agg({'magnitude':'avg'})\n",
    "average_magnitude_df.select(average_magnitude_df['avg(magnitude)'])\n",
    "average_magnitude_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o173.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ff1a35fbe706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m average_magnitude_df.write.format('jdbc').options(url='jdbc:postgresql://localhost:5432/kafka', driver= 'org.postgresql.Driver',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                     dbtable='earthquake_table_1', user='memosha',password='1234').mode('overwrite').save()\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o173.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "average_magnitude_df.write.format('jdbc').options(url='jdbc:postgresql://localhost:5432/kafka', driver= 'org.postgresql.Driver',\n",
    "                                                    dbtable='earthquake_table_1', user='memosha',password='1234').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Highest magnitude for each country in the month of November in 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+-----------------+\n",
      "|year|month|            location|highest_magnitude|\n",
      "+----+-----+--------------------+-----------------+\n",
      "|2022|   11|Bahía de Kino, Me...|                6|\n",
      "|2022|   11|Colorado City, Ar...|                3|\n",
      "|2022|   11|       Corinne, Utah|                3|\n",
      "|2022|   11|     Dubois, Wyoming|                3|\n",
      "|2022|   11|  Lake Pillsbury, CA|                3|\n",
      "|2022|   11| Maneadero, B.C., MX|                4|\n",
      "|2022|   11|      Mentone, Texas|                5|\n",
      "|2022|   11|    Ravalli, Montana|                3|\n",
      "|2022|   11|      Stanley, Idaho|                3|\n",
      "|2022|   11|     Taylor, Wyoming|                3|\n",
      "|2022|   11|        Toyah, Texas|                3|\n",
      "|2022|   11|Whites City, New ...|                3|\n",
      "+----+-----+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_magnitude_november_df = earthquake_df\\\n",
    "                    .groupBy(F.year('date').alias('year'), F.month('date').alias('month'), 'location')\\\n",
    "                    .agg(F.max('magnitude').alias('highest_magnitude'))\\\n",
    "                    .filter('month == 11')\\\n",
    "                    .filter('year == 2022')\\\n",
    "                    .orderBy('location', 'year')\n",
    "\n",
    "highest_magnitude_november_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o155.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4df5774ab7fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m highest_magnitude_november_df.write.format('jdbc').options(url='jdbc:postgresql://localhost:5432/kafka', driver= 'org.postgresql.Driver',\n\u001b[0m\u001b[1;32m      2\u001b[0m dbtable='earthquake_table_1', user='memosha',password='1234').mode('overwrite').save()\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o155.save.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "highest_magnitude_november_df.write.format('jdbc').options(url='jdbc:postgresql://localhost:5432/kafka', driver= 'org.postgresql.Driver',\n",
    "dbtable='earthquake_table_1', user='memosha',password='1234').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find the status of earthquake on the basis of each place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               place|    status|\n",
      "+--------------------+----------+\n",
      "|100km S of San Cl...|[reviewed]|\n",
      "|12km WSW of Petro...|[reviewed]|\n",
      "|13km NNW of Grape...|[reviewed]|\n",
      "|13km SE of Bodfis...|[reviewed]|\n",
      "|14km ESE of Alum ...|[reviewed]|\n",
      "|14km ESE of Woffo...|[reviewed]|\n",
      "|14km SE of Baysid...|[reviewed]|\n",
      "|15km E of Seven T...|[reviewed]|\n",
      "|15km ESE of Alum ...|[reviewed]|\n",
      "|16 km ESE of Laco...|[reviewed]|\n",
      "|16km WNW of Lake ...|[reviewed]|\n",
      "|16km WSW of Weitc...|[reviewed]|\n",
      "|17 km NNW of Tayl...|[reviewed]|\n",
      "|21 km NW of Bickl...|[reviewed]|\n",
      "|21 km SSE of Golf...|[reviewed]|\n",
      "|22 km SSW of Mamm...|[reviewed]|\n",
      "|23km E of Julian, CA|[reviewed]|\n",
      "|23km NE of Therma...|[reviewed]|\n",
      "|24 km NW of Stanl...|[reviewed]|\n",
      "|25km W of Petroli...|[reviewed]|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_place_df = earthquake_df.withColumn('status',F.collect_set('status').over(W.partitionBy('place'))).select('place', 'status').distinct()\n",
    "status_place_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
